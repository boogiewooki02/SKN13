{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4766ebb-432c-4bdb-8050-23df797098be",
   "metadata": {},
   "source": [
    "# Langchain은 다양한 LLM(대규모 언어 모델)을 지원한다\n",
    "-\t대규모 언어 모델(LLM, Large Language Model)을 개발하는 회사들은 사용자가 자신의 애플리케이션에서 LLM을 손쉽게 활용할 수 있도록 API(Application Programming Interface) 서비스를 제공하고 있다.\n",
    "-\t하지만 각 LLM은 고유한 API 호출 라이브러리(Library)를 제공하기 때문에, 개발자는 동일한 작업을 수행하더라도 LLM에 따라 다른 코드를 작성해야 하는 번거로움이 있다.\n",
    "-\tLangchain은 이러한 문제를 해결하기 위해 다양한 LLM의 API를 통합적으로 지원한다.\n",
    "-\t여러 LLM을 동일한 인터페이스(interface)로 호출할 수 있게 하여 특정 모델에 종속되지 않도록 하고, 필요에 따라 쉽게 다른 모델로 전환할 수 있다.\n",
    "-\tLangchain이 지원하는 주요 LLM 목록\n",
    "    - https://python.langchain.com/docs/integrations/chat/#featured-providers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e3b65a-1b4e-4fcc-99ac-ec7624dac9ad",
   "metadata": {},
   "source": [
    "## 설치\n",
    "```bash\n",
    "pip install langchain langchain_core langchain-community  -qU\n",
    "pip install python-dotenv -qU \n",
    "pip install ipywidgets -qU\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb9e5829-972a-4345-858d-e479dab7c320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.3.25'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain\n",
    "\n",
    "langchain.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9025ef1-ede0-4f3b-b8fe-1b9e348391ba",
   "metadata": {},
   "source": [
    "# OpenAI 모델 사용\n",
    "- https://platform.openai.com\n",
    "  \n",
    "## 결제\n",
    "1. 로그인 후 Billing 페이지로 이동.\n",
    "   - setting -> Billing\n",
    "  \n",
    "   ![openai_payment.png](figures/openai_payment.png)\n",
    "\n",
    "2. Payment methods 탭을 선택하고 카드를 등록한다. \n",
    "   \n",
    "   ![openai_payment2.png](figures/openai_payment2.png)\n",
    "\n",
    "   - 등록이 끝나면 최초 구매를 진행한다. $5 ~ $100 사이의 금액을 선택할 수 있다.\n",
    "   - 자동 충전을 설정하고 싶다면 automatic recharge 를 활성화 하고 아래 추가 설정에 입력한다. \n",
    "     - 자동 충전은 특정 금액 이하로 떨어지면 자동으로 충전한다. (**비활성화**) \n",
    "  \n",
    "   ![openai_payment3.png](figures/openai_payment3.png)\n",
    "   \n",
    "3. 수동으로 **추가 결제하기**\n",
    "   - Billing 페이지의 Overview에서 `Add to credit balance` 를 클릭한 뒤 금액을 입력하고 결제한다.\n",
    "\n",
    "## 사용량 확인\n",
    "- profile/설정 -> Usage 에서 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798f99e0-d15b-4769-8d2c-b6e0a9a26237",
   "metadata": {},
   "source": [
    "## API Key 생성\n",
    "  \n",
    "![openai_create_apikey.png](figures/openai_create_apikey.png)\n",
    "\n",
    "- 로그인 -> Dashboard -> API Keys -> Create New Secreat Key\n",
    "> Settings -> API Keys\n",
    "\n",
    "## API Key 등록\n",
    "- 환경변수에 등록\n",
    "  - 변수이름: OPENAI_API_KEY\n",
    "  - 값: 생성된 키\n",
    "- dotenv를 이용해서 load\n",
    "  - Working directory에  `.env` 파일 생성하고 `OPENAI_API_KEY=생성된키` 추가한다.\n",
    "  - load_dotenv() 호출 하면 .env 파일에 있는 값을 읽은 뒤 환경변수로 등록한다.\n",
    "- **주의**\n",
    "  - 생성된 API Key는 노출되면 안된다.\n",
    "  - API Key가 저장된 파일(코드나 설정파일)이 github에 올라가 공개되서는 안된다.\n",
    "\n",
    "## 사용 비용 확인\n",
    "- settings -> Usage 에서 확인\n",
    "\n",
    "## OpenAI LLM 모델들\n",
    "-  OpenAI LLM 모델: https://platform.openai.com/docs/models\n",
    "-  모델별 가격: https://platform.openai.com/docs/pricing\n",
    "-  토큰사이즈 확인: https://platform.openai.com/tokenizer\n",
    "   -  1토큰: 영어 3\\~4글자 정도, 한글: 대략 1\\~2글자 정도\n",
    "   -  모델이 업데이트 되면서 토큰 사이즈도 조금씩 커지고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c5a12d-8e7d-4a29-ac91-edb62c56bfe7",
   "metadata": {},
   "source": [
    "## OpenAI 를 연동하기 위한 package 설치\n",
    "```bash\n",
    "pip install langchain-openai -qU\n",
    "```\n",
    "\n",
    "- OpenAI 자체 라이브러리 설치\n",
    "    - `pip install openai -qU`\n",
    "    - langchain-openai를 설치하면 같이 설치 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29772e79-1e31-421d-b5c1-625dc029c395",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "flag = load_dotenv() # 임시적으로 .env 내의 key를 환경변수로 등록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc1d55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "message = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"OpenAI의 LLM 모델은 무엇이 있나요?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create( # 요청 -> 응답 반환\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=message\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5f774f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-BetlMrI2WrIPfSyuHovZdluVAC77q', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='OpenAI의 LLM(대형 언어 모델)에는 여러 가지 버전과 변형이 있습니다. 여기 몇 가지 주요 모델을 소개합니다:\\n\\n1. **GPT-2**: OpenAI가 처음으로 발표한 대형 언어 모델로, 다양한 자연어 처리 작업에서 뛰어난 성능을 보여줍니다.\\n\\n2. **GPT-3**: GPT-2의 후속 모델로, 더 많은 파라미터(1750억)를 가지고 있으며, 다양한 텍스트 생성 작업에서 매우 뛰어난 성능을 보입니다. API를 통해 상업적으로 이용될 수 있습니다.\\n\\n3. **GPT-3.5**: GPT-3의 개선된 버전으로, 더 나은 응답 품질과 다양한 작업에서의 안정성을 제공합니다.\\n\\n4. **GPT-4**: 최신 모델로, GPT-3.5보다 더욱 발전된 성능을 가지고 있습니다. 더 높은 정확성과 다양한 작업에 대한 응답 능력을 갖추고 있습니다.\\n\\n이 모델들은 자연어 이해, 자연어 생성, 요약, 번역, 질의응답 등 다양한 언어 관련 작업에 활용될 수 있습니다. OpenAI는 연구와 개발을 통해 지속적으로 이러한 모델들을 발전시키고 있습니다.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1749087172, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_62a23a81ef', usage=CompletionUsage(completion_tokens=259, prompt_tokens=19, total_tokens=278, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e4454f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI의 LLM(대형 언어 모델)에는 여러 가지 버전과 변형이 있습니다. 여기 몇 가지 주요 모델을 소개합니다:\n",
      "\n",
      "1. **GPT-2**: OpenAI가 처음으로 발표한 대형 언어 모델로, 다양한 자연어 처리 작업에서 뛰어난 성능을 보여줍니다.\n",
      "\n",
      "2. **GPT-3**: GPT-2의 후속 모델로, 더 많은 파라미터(1750억)를 가지고 있으며, 다양한 텍스트 생성 작업에서 매우 뛰어난 성능을 보입니다. API를 통해 상업적으로 이용될 수 있습니다.\n",
      "\n",
      "3. **GPT-3.5**: GPT-3의 개선된 버전으로, 더 나은 응답 품질과 다양한 작업에서의 안정성을 제공합니다.\n",
      "\n",
      "4. **GPT-4**: 최신 모델로, GPT-3.5보다 더욱 발전된 성능을 가지고 있습니다. 더 높은 정확성과 다양한 작업에 대한 응답 능력을 갖추고 있습니다.\n",
      "\n",
      "이 모델들은 자연어 이해, 자연어 생성, 요약, 번역, 질의응답 등 다양한 언어 관련 작업에 활용될 수 있습니다. OpenAI는 연구와 개발을 통해 지속적으로 이러한 모델들을 발전시키고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content) # 마크다운 형식임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b839f7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "OpenAI의 LLM(대형 언어 모델)에는 여러 가지 버전과 변형이 있습니다. 여기 몇 가지 주요 모델을 소개합니다:\n",
       "\n",
       "1. **GPT-2**: OpenAI가 처음으로 발표한 대형 언어 모델로, 다양한 자연어 처리 작업에서 뛰어난 성능을 보여줍니다.\n",
       "\n",
       "2. **GPT-3**: GPT-2의 후속 모델로, 더 많은 파라미터(1750억)를 가지고 있으며, 다양한 텍스트 생성 작업에서 매우 뛰어난 성능을 보입니다. API를 통해 상업적으로 이용될 수 있습니다.\n",
       "\n",
       "3. **GPT-3.5**: GPT-3의 개선된 버전으로, 더 나은 응답 품질과 다양한 작업에서의 안정성을 제공합니다.\n",
       "\n",
       "4. **GPT-4**: 최신 모델로, GPT-3.5보다 더욱 발전된 성능을 가지고 있습니다. 더 높은 정확성과 다양한 작업에 대한 응답 능력을 갖추고 있습니다.\n",
       "\n",
       "이 모델들은 자연어 이해, 자연어 생성, 요약, 번역, 질의응답 등 다양한 언어 관련 작업에 활용될 수 있습니다. OpenAI는 연구와 개발을 통해 지속적으로 이러한 모델들을 발전시키고 있습니다."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 주피터에서 마크다운 형식으로 확인해보기\n",
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a486e5a1-6c59-499d-a275-41060c8b0a8a",
   "metadata": {},
   "source": [
    "## OpenAI Library 를 이용한 API 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9bb769-eb14-43d5-9c26-98bb77153b48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851be9d2-58e5-464b-87cb-52f09a9a146b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7056dd-ddc5-4c0f-8653-47be64287d4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f77416bf-be44-421b-bed1-c1da8512e983",
   "metadata": {},
   "source": [
    "## Langchain을 이용한 OpenAI API 호출\n",
    "\n",
    "- **ChatOpenAI**\n",
    "    - chat (대화-채팅) 기반 모델 model.\n",
    "    - Default 로 gpt-3.5-turbo 사용\n",
    "    - llm 전달 입력과 llm 응답 출력 타입:  Message\n",
    "> - **OpenAI**\n",
    ">     - 문장 완성 모델. (text completion) model\n",
    ">     - Default로 gpt-3.5-turbo-instruct 사용\n",
    ">       - instruct 모델만 사용가능\n",
    ">     - llm전달 입력과 llm 응답 출력 타입: str\n",
    "- Initializer 주요 파라미터\n",
    "    -  **temperature**\n",
    "        -  llm 모델의 출력 무작위성을 지정한다. \n",
    "        -  0 ~ 2 사이 실수를 설정하며 클 수록 무작위성이 커진다. 기본값: 0.7\n",
    "        -  정확한 답변을 얻어야 하는 경우 작은 값을 창작을 해야 하는 경우 큰 값을 지정한다.\n",
    "    -  **model_name**\n",
    "        -  사용할 openai 모델 지정\n",
    "    - **max_tokens**:\n",
    "        - llm 모델이 응답할 최대 token 수.\n",
    "    - **api_key**\n",
    "        - OpenAI API key를 직접 입력해 생성시 사용.\n",
    "        - API key가 환경변수에 설정 되있으면 생략한다. \n",
    "-  메소드\n",
    "    - **`invoke(message)`** : LLM에 질의 메세지를 전달하며 LLM의 응답을 반환한다.\n",
    "> - **Message**\n",
    ">     - Langchain 다양한 상황과 작업 마다 다양한 값들로 구성된 입출력 데이터를 만든다. \n",
    ">     - Langchain은 그 상황들에 맞는 다양한 Message 클래스를 제공한다. 이것을 이용하면 특정 작업에 적합한 입력값을 설정할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "124af8a3-d624-4399-8062-35a6d60c63b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1948856-b1e4-4da0-9311-11f798b66a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\"\n",
    ")\n",
    "\n",
    "response = model.invoke(\"OpenAI LLM 모델 종류를 알려줘.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afed81fb-5249-4bd3-91b0-4f86a0a75f5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='OpenAI의 LLM(대규모 언어 모델)에는 여러 가지 모델이 있습니다. 주로 알려진 모델로는 다음과 같은 것들이 있습니다:\\n\\n1. **GPT-3**: Generative Pre-trained Transformer 3은 다양한 자연어 처리 작업에서 높은 성능을 발휘하는 대규모 모델입니다. 다양한 크기(예: Davinci, Curie, Babbage, Ada)로 제공됩니다.\\n\\n2. **GPT-4**: GPT-3의 후속 모델로, 더 발전된 기능과 성능을 갖추고 있습니다. 텍스트 생성, 질문 응답, 번역 등 다양한 작업에서 사용됩니다.\\n\\n3. **Codex**: 프로그래밍 언어를 이해하고 코드 작성, 코드 수정 등을 수행할 수 있는 모델로, GitHub Copilot 등 개발 도구에 통합되어 사용됩니다.\\n\\n이 외에도 OpenAI에서는 다양한 연구 및 실험을 통해 여러 형태의 모델을 개발하고 있으며, 최신 정보는 OpenAI 공식 웹사이트나 블로그를 통해 확인할 수 있습니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 222, 'prompt_tokens': 17, 'total_tokens': 239, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_62a23a81ef', 'id': 'chatcmpl-Betu9aLnkexRyVGqyeFamT7ZO7ghW', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--52c3604b-95fc-407d-9799-d9c431df26c5-0', usage_metadata={'input_tokens': 17, 'output_tokens': 222, 'total_tokens': 239, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4afb3b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI의 LLM(대규모 언어 모델)에는 여러 가지 모델이 있습니다. 주로 알려진 모델로는 다음과 같은 것들이 있습니다:\n",
      "\n",
      "1. **GPT-3**: Generative Pre-trained Transformer 3은 다양한 자연어 처리 작업에서 높은 성능을 발휘하는 대규모 모델입니다. 다양한 크기(예: Davinci, Curie, Babbage, Ada)로 제공됩니다.\n",
      "\n",
      "2. **GPT-4**: GPT-3의 후속 모델로, 더 발전된 기능과 성능을 갖추고 있습니다. 텍스트 생성, 질문 응답, 번역 등 다양한 작업에서 사용됩니다.\n",
      "\n",
      "3. **Codex**: 프로그래밍 언어를 이해하고 코드 작성, 코드 수정 등을 수행할 수 있는 모델로, GitHub Copilot 등 개발 도구에 통합되어 사용됩니다.\n",
      "\n",
      "이 외에도 OpenAI에서는 다양한 연구 및 실험을 통해 여러 형태의 모델을 개발하고 있으며, 최신 정보는 OpenAI 공식 웹사이트나 블로그를 통해 확인할 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b41a3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0, # 무작위성을 설정(0 이상의 실수, 보통 0~1). 값이 클 수록 무작위성이 커진다.\n",
    "    # max_completion_tokens=100, # 응답 토큰 수 제한\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bea4cd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "    태양계를 구성하는 행성들의 이름을 태양에서 가까운 순서대로 알려줘.\n",
    "\n",
    "    <답변 형식>\n",
    "    목록 형식으로 답변해줘.\n",
    "    - 한글이름(영어이름): 행성에 대한 간단한 설명\n",
    "\"\"\"\n",
    "res = model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f05b195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 수성(Mercury): 태양에 가장 가까운 행성으로, 대기가 거의 없어 낮과 밤의 온도 차이가 매우 큽니다.\n",
      "- 금성(Venus): 지구와 비슷한 크기를 가진 행성이지만, 두꺼운 이산화탄소 대기로 인해 극심한 온실 효과가 발생합니다.\n",
      "- 지구(Earth): 생명체가 존재하는 유일한 행성으로, 물이 액체 상태로 존재할 수 있는 조건을 갖추고 있습니다.\n",
      "- 화성(Mars): 붉은 색을 띠는 행성으로, 과거에 물이 존재했을 가능성이 있으며, 현재 탐사 missions이 활발히 진행되고 있습니다.\n",
      "- 목성(Jupiter): 태양계에서 가장 큰 행성으로, 강력한 자기장을 가지고 있으며, 수많은 위성을 거느리고 있습니다.\n",
      "- 토성(Saturn): 아름다운 고리로 유명한 행성으로, 가스 행성 중 하나이며, 많은 위성을 가지고 있습니다.\n",
      "- 천왕성(Uranus): 독특하게 옆으로 기울어진 축을 가진 행성으로, 푸른색의 대기와 차가운 온도를 특징으로 합니다.\n",
      "- 해왕성(Neptune): 태양계에서 가장 먼 행성으로, 강한 바람과 대기의 푸른색이 특징이며, 여러 개의 위성을 가지고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "acb5ceee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 채팅 형태 프롬프트 - role과 내용을 묶어서 전달\n",
    "prompt = [\n",
    "    (\"user\", \"막걸리 제조법을 알려줘.\") # (role, 대화내용)\n",
    "]\n",
    "res = model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "32018f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='막걸리는 한국의 전통 발효주로, 쌀과 물, 누룩을 주재료로 사용하여 만듭니다. 아래는 기본적인 막걸리 제조법입니다.\\n\\n### 재료\\n- 쌀: 1kg (찹쌀 또는 일반 쌀)\\n- 물: 약 1.5~2리터\\n- 누룩: 100g (막걸리 누룩 또는 일반 누룩)\\n- 설탕: 선택 사항 (단맛을 원할 경우)\\n\\n### 제조 과정\\n\\n1. **쌀 씻기**: 쌀을 깨끗이 씻어 불순물을 제거합니다. 여러 번 물을 갈아주며 씻어주세요.\\n\\n2. **쌀 불리기**: 씻은 쌀을 물에 담가 4~6시간 정도 불립니다. 찹쌀을 사용할 경우, 더 부드럽고 쫄깃한 막걸리가 됩니다.\\n\\n3. **쌀 찌기**: 불린 쌀을 체에 받쳐 물기를 빼고, 찜통에 넣어 30~40분 정도 쪄줍니다. 쌀이 고루 익도록 중간에 한 번 저어주는 것이 좋습니다.\\n\\n4. **식히기**: 찐 쌀을 넓은 그릇에 옮겨 담고, 상온에서 식힙니다. 온도가 30도 정도로 식어야 합니다.\\n\\n5. **누룩 섞기**: 식은 쌀에 누룩을 고루 섞어줍니다. 누룩이 잘 섞이도록 손이나 주걱을 사용하세요.\\n\\n6. **발효**: 혼합물을 깨끗한 발효 용기에 옮기고, 물을 추가합니다. 뚜껑을 덮되, 완전히 밀봉하지 않고 공기가 통할 수 있도록 합니다. 1~2일 동안 실온에서 발효시킵니다. 발효가 진행되면 거품이 생기고, 향이 나기 시작합니다.\\n\\n7. **여과**: 발효가 끝나면, 체나 면포를 사용해 막걸리를 걸러냅니다. 이때, 남은 찌꺼기는 버리거나 다른 요리에 활용할 수 있습니다.\\n\\n8. **병입 및 저장**: 걸러낸 막걸리를 깨끗한 병에 담고, 냉장고에 보관합니다. 이때, 원한다면 설탕을 추가하여 단맛을 조절할 수 있습니다.\\n\\n9. **숙성**: 막걸리는 냉장고에서 며칠 숙성시키면 더욱 맛이 좋아집니다. \\n\\n이제 막걸리를 즐길 준비가 완료되었습니다! 막걸리는 차갑게 해서 마시는 것이 일반적이며, 다양한 안주와 함께 즐기면 좋습니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 621, 'prompt_tokens': 16, 'total_tokens': 637, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_62a23a81ef', 'id': 'chatcmpl-BeuX73NQkbBBjHhdBInRFhtSaN89D', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--297595cc-a4f5-4442-9ee7-3775ed3aa852-0', usage_metadata={'input_tokens': 16, 'output_tokens': 621, 'total_tokens': 637, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c2c5ea87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "막걸리는 한국의 전통 발효주로, 쌀과 물, 누룩을 주재료로 사용하여 만듭니다. 아래는 기본적인 막걸리 제조법입니다.\n",
      "\n",
      "### 재료\n",
      "- 쌀: 1kg (찹쌀 또는 일반 쌀)\n",
      "- 물: 약 1.5~2리터\n",
      "- 누룩: 100g (막걸리 누룩 또는 일반 누룩)\n",
      "- 설탕: 선택 사항 (단맛을 원할 경우)\n",
      "\n",
      "### 제조 과정\n",
      "\n",
      "1. **쌀 씻기**: 쌀을 깨끗이 씻어 불순물을 제거합니다. 여러 번 물을 갈아주며 씻어주세요.\n",
      "\n",
      "2. **쌀 불리기**: 씻은 쌀을 물에 담가 4~6시간 정도 불립니다. 찹쌀을 사용할 경우, 더 부드럽고 쫄깃한 막걸리가 됩니다.\n",
      "\n",
      "3. **쌀 찌기**: 불린 쌀을 체에 받쳐 물기를 빼고, 찜통에 넣어 30~40분 정도 쪄줍니다. 쌀이 고루 익도록 중간에 한 번 저어주는 것이 좋습니다.\n",
      "\n",
      "4. **식히기**: 찐 쌀을 넓은 그릇에 옮겨 담고, 상온에서 식힙니다. 온도가 30도 정도로 식어야 합니다.\n",
      "\n",
      "5. **누룩 섞기**: 식은 쌀에 누룩을 고루 섞어줍니다. 누룩이 잘 섞이도록 손이나 주걱을 사용하세요.\n",
      "\n",
      "6. **발효**: 혼합물을 깨끗한 발효 용기에 옮기고, 물을 추가합니다. 뚜껑을 덮되, 완전히 밀봉하지 않고 공기가 통할 수 있도록 합니다. 1~2일 동안 실온에서 발효시킵니다. 발효가 진행되면 거품이 생기고, 향이 나기 시작합니다.\n",
      "\n",
      "7. **여과**: 발효가 끝나면, 체나 면포를 사용해 막걸리를 걸러냅니다. 이때, 남은 찌꺼기는 버리거나 다른 요리에 활용할 수 있습니다.\n",
      "\n",
      "8. **병입 및 저장**: 걸러낸 막걸리를 깨끗한 병에 담고, 냉장고에 보관합니다. 이때, 원한다면 설탕을 추가하여 단맛을 조절할 수 있습니다.\n",
      "\n",
      "9. **숙성**: 막걸리는 냉장고에서 며칠 숙성시키면 더욱 맛이 좋아집니다. \n",
      "\n",
      "이제 막걸리를 즐길 준비가 완료되었습니다! 막걸리는 차갑게 해서 마시는 것이 일반적이며, 다양한 안주와 함께 즐기면 좋습니다.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b395c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'completion_tokens': 621,\n",
       "  'prompt_tokens': 16,\n",
       "  'total_tokens': 637,\n",
       "  'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
       "   'audio_tokens': 0,\n",
       "   'reasoning_tokens': 0,\n",
       "   'rejected_prediction_tokens': 0},\n",
       "  'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}},\n",
       " 'model_name': 'gpt-4o-mini-2024-07-18',\n",
       " 'system_fingerprint': 'fp_62a23a81ef',\n",
       " 'id': 'chatcmpl-BeuX73NQkbBBjHhdBInRFhtSaN89D',\n",
       " 'service_tier': 'default',\n",
       " 'finish_reason': 'stop',\n",
       " 'logprobs': None}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.response_metadata # 응답에 대한 정보"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71861354-39b6-4a7d-842d-8939d3a3e5bc",
   "metadata": {},
   "source": [
    "# Hugging Face 모델 사용\n",
    "\n",
    "## Local 에 설치된 모델 사용\n",
    "- HuggingFacePipeline 에 Model id를 전달해 Model객체를 생성한다.\n",
    "- huggingface transformers 라이브러리를 이용해 model을 생성 한 뒤 HuggingFacePipeline 에 넣어 생성한다.\n",
    "- 모델이 local에 없는 경우 다운로드 받는다.\n",
    "\n",
    "### HuggingFace 모델을 사용하기 위한 package 설치\n",
    "```bash\n",
    "pip install transformers -qU\n",
    "pip install langchain-huggingface -qU\n",
    "pip install  huggingface_hub -qU\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "49fb8a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "login(hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1a6c8929-6c61-4e85-95b9-40857fd6bff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5632e105339b43f99b392d70820a25a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata\\anaconda3\\envs\\lang_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Playdata\\.cache\\huggingface\\hub\\models--google--gemma-3-1b-it. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbaafa28807c42f099062874df24b96f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ea6cde1d2cf41d1b1c96264603a4566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9b80418afc04758b7c7e44a0e996338",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98ddc168232444559e536e43f446c7f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bb3a716378446f8b5234af885cd7954",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/899 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dad67af1a5e48eab559929b01761e99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "566611502a634e0cbeb3d7016b3a1ac2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "\n",
    "model_id = \"google/gemma-3-1b-it\" # 1b: 파라미터 수 (10억개), it(instruction training): 질문-응답 형식으로 파인튜닝한 모델\n",
    "model_hf = HuggingFacePipeline.from_model_id(\n",
    "    model_id=model_id,\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\"max_new_tokens\": 50} # transformers.pipeline()의 설정을 하는 파라미터\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7562fc5a-6c5f-456b-aafd-ec8dbf9ef54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model_hf.invoke(\"한국의 수도는 어디인가요?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "37832a44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'한국의 수도는 어디인가요?\\n\\n정답은 서울입니다.\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b653f31b-82aa-462f-a140-545b5c57d485",
   "metadata": {},
   "source": [
    "# Anthropic의 Claude 모델 사용\n",
    "\n",
    "- Anthropic사의 Claude 모델은 (성능 순으로) **Haiku, Sonnet, Opus** 세가지 모델이 있다.  \n",
    "- [Anthropic사 사이트](https://www.anthropic.com/)\n",
    "- [Claude 서비스 사이트](https://claude.ai)\n",
    "- API 가격: https://docs.anthropic.com/en/docs/about-claude/pricing\n",
    "- Langchain으로 Anthropic claude 모델 사용: https://python.langchain.com/docs/integrations/chat/anthropic/\n",
    "\n",
    "## API Key 발급받기\n",
    "1. https://console.anthropic.com/ 이동 후 가입한다.\n",
    "2. 로그인 하면 Dashboard로 이동한다. Dashbord에서 `Get API Keys`를 클릭해 이동한다.\n",
    "\n",
    "![anthropic_apikey1.png](figures/anthropic_apikey1.png)\n",
    "\n",
    "3. Create key 클릭해서 API Key를 생성한다.\n",
    "\n",
    "![anthropic_apikey2.png](figures/anthropic_apikey2.png)\n",
    "\n",
    "4. 생성된 API Key를 복사한 뒤 저장. (다시 볼 수 없다.)\n",
    "   - 환경변수에 등록\n",
    "      - 변수이름: ANTHROPIC_API_KEY\n",
    "      - 값: 생성된 키\n",
    "5. 결제 정보 등록 및 결제 (최소 $5)\n",
    "   - Settings -> Billing -> complete setup\n",
    "  \n",
    "![anthropic_apikey3.png](figures/anthropic_apikey3.png)\n",
    "  - 설문조사 후 카드 등록한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8174de-b6a1-423b-8cce-271917ae8dc6",
   "metadata": {},
   "source": [
    "## Anthropic의 Claude 모델 사용\n",
    "- 모델 확인: https://docs.anthropic.com/en/docs/about-claude/models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8193ce-fa8b-4208-a7d9-af794044e61c",
   "metadata": {},
   "source": [
    "### Claude 모델 사용을 위한 package 설치\n",
    "\n",
    "```bash\n",
    "pip install langchain-anthropic -qU\n",
    "pip install anthropic -qU\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b1e3a23f-c491-4245-9013-83c5706fd4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install anthropic -qU\n",
    "%pip install -qU langchain-anthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0298545f-64b9-41aa-a375-7296030bb108",
   "metadata": {},
   "source": [
    "### Langchain-antropic 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba755283-5d24-464d-8c81-21d496100256",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic# , Anthropic 지원하는 모델이 다른 것 같다.\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "model = \"claude-3-5-haiku-latest\"\n",
    "# model = \"claude-3-5-sonnet-latest\"\n",
    "llm = ChatAnthropic(\n",
    "    model=model,\n",
    "    temperature=0.2,\n",
    "    max_tokens=1024,\n",
    ")\n",
    "result = llm.invoke(\"Anthropic의 LLM 모델은 어떤 것이 있는지 알려주고 간단한 설명도 부탁해.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1684d4b-a5f9-4e24-8263-b30d997833b0",
   "metadata": {},
   "source": [
    "# Ollama 모델 사용\n",
    "\n",
    "Ollama는 로컬 환경에서 오픈소스 LLM을 쉽게 실행할 수 있도록 지원하는 플랫폼이다.\n",
    "\n",
    "- 주요특징\n",
    "\n",
    "  - **다양한 모델 지원**: Llama 3, Mistral, Phi 3 등 여러 오픈소스 LLM을 지원.\n",
    "  - **편리한 모델 설치 및 실행**: 간단한 명령어로 모델을 다운로드하고 실행할 수 있습니다.\n",
    "  - **운영체제 호환성**: macOS, Windows, Linux 등 다양한 운영체제에서 사용 가능하다.\n",
    "\n",
    "## 설치\n",
    "- https://ollama.com/download 에서 운영체제에 맞는 버전을 설치\n",
    "-  Windows 버전은 특별한 설정 없이 바로 install 실행하면 된다.\n",
    "\n",
    "## 모델 검색\n",
    "- https://ollama.com/search\n",
    "- 모델을 검색한 후 상세페이지로 이동하면 해당 모델을 실행할 수있는 명령어가 나온다.\n",
    "\n",
    "![ollama_down.png](figures/ollama_down.png)\n",
    "\n",
    "\n",
    "## 실행 명령어\n",
    "- `ollama pull 모델명`\n",
    "  - 모델을 다운로드 받는다. (다운로드만 받고 실행은 하지 않은다.)\n",
    "- `ollama run 모델명`\n",
    "  - 모델을 실행한다. \n",
    "  - 최초 실행시 모델을 다운로드 받는다.\n",
    "  - 명령프롬프트 상에서 `프롬프트`를 입력하면 모델의 응답을 받을 수 있다.\n",
    "\n",
    "## Python/Langchain API\n",
    "- ollama api\n",
    "  - https://github.com/ollama/ollama-python\n",
    "- langchain-ollama\n",
    "  - https://python.langchain.com/docs/integrations/chat/ollama/\n",
    "- 설치\n",
    "  - `pip install langchain-ollama`\n",
    "  - `pip install ollama`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b0bf62",
   "metadata": {},
   "source": [
    "GUI 환경: open-webui 설치"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2c2254-7dd3-48b1-9de4-d25e278a2266",
   "metadata": {},
   "source": [
    "## Langchain-ollama 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0410c58d-5d0d-42a8-a2d6-0dfa0f7772f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model_id = \"qwen3:0.6b\"\n",
    "model = ChatOllama(model=model_id)\n",
    "response = model.invoke(\"대한민국 수도의 이름은?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ffd2dbcc-d3f6-4e30-86dd-f33db4f69a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user is asking for the name of the capital of the Republic of Korea. I know that the Republic of Korea is a country in East Asia, and the capital is Seoul. But wait, I need to make sure I'm not confusing it with another country. Let me think... The capital of South Korea is indeed Seoul, right? And there's no other capital mentioned here. So the answer should be Seoul. But just to double-check, maybe I should confirm the official name. Yes, Seoul is the official name. No other cities or states are involved here. So the answer is Seoul.\n",
      "</think>\n",
      "\n",
      "대한민국의 수도는 **서울**입니다.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832b1cd0-e201-478f-85ae-f7ab40c778dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "588b1d44",
   "metadata": {},
   "source": [
    "# Gemini\n",
    "- 모델: https://ai.google.dev/gemini-api/docs/models?hl=ko\n",
    "- 가격정책: https://ai.google.dev/gemini-api/docs/pricing?hl=ko\n",
    "\n",
    "## API Key 생성\n",
    "\n",
    "1. https://aistudio.google.com/\n",
    "    - 연결 후 로그인(구글계정)\n",
    "2. Get API Key 클릭\n",
    "   \n",
    "    ![img](figures/gemini_api1.png)\n",
    "\n",
    "3. `API Key 만들기` 선택\n",
    "4. 프로젝트 선택 후 `기존 프로젝트에서 API 키 만들기` 선택\n",
    "\n",
    "## 환경변수\n",
    "- `GOOGLE_API_KEY` 환경변수에 생성된 API Key를 등록한다.\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9b2e8fd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "18500ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "model_id = \"gemini-2.5-flash-preview-05-20\"\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=model_id\n",
    ")\n",
    "\n",
    "response = model.invoke(\"gemini와 gemma의 차이는?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6e9171b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini와 Gemma는 모두 Google이 개발한 AI 모델이지만, **목표, 규모, 활용 방식, 그리고 라이선스에서 큰 차이**가 있습니다. 비유하자면, Gemini는 '최고급 플래그십 전투기'이고, Gemma는 '개발자를 위한 고성능 미니카'라고 할 수 있습니다.\n",
      "\n",
      "자세한 차이점은 다음과 같습니다.\n",
      "\n",
      "---\n",
      "\n",
      "### 1. Gemini (제미니)\n",
      "\n",
      "*   **개발 목적:** Google의 가장 강력하고 최첨단 AI 모델로, **다양한 복잡한 작업을 수행하는 범용 인공지능**을 목표로 합니다.\n",
      "*   **모델 크기/규모:** 매우 크고 복잡한 모델로, 여러 가지 크기(Nano, Pro, Ultra)가 있지만, 일반적으로 **최고 성능을 위한 대규모 모델**을 지칭합니다.\n",
      "*   **주요 특징:**\n",
      "    *   **멀티모달(Multimodal):** 텍스트, 이미지, 오디오, 비디오 등 다양한 형태의 정보를 이해하고 생성할 수 있습니다.\n",
      "    *   **최고 성능:** 현존하는 AI 모델 중 최고 수준의 성능을 자랑하며, 복잡한 추론, 코딩, 창의적인 콘텐츠 생성 등에 강합니다.\n",
      "    *   **폐쇄형/API 접근:** 주로 Google의 서비스(예: Gemini Advanced, Google Cloud Vertex AI)를 통해 API 형태로 접근하거나, Google 제품에 통합되어 사용됩니다. 모델의 가중치 등 세부 정보는 공개되지 않습니다.\n",
      "*   **주요 활용 분야:** Google의 AI 기반 서비스(Bard/Gemini Advanced), 복잡한 기업용 애플리케이션, 연구 개발의 최전선.\n",
      "*   **비유:** Google의 **주력 전투기** 또는 **최고급 스포츠카**.\n",
      "\n",
      "### 2. Gemma (젬마)\n",
      "\n",
      "*   **개발 목적:** Google이 개발한 **경량, 오픈 소스 AI 모델 패밀리**입니다. 개발자와 연구자들이 쉽게 접근하고 활용하며, 특정 용도에 맞게 미세 조정(fine-tuning)할 수 있도록 설계되었습니다.\n",
      "*   **모델 크기/규모:** Gemini보다 훨씬 작은 규모로 제공됩니다 (예: 2B, 7B). 이는 **효율성과 접근성**에 중점을 둔 결과입니다.\n",
      "*   **주요 특징:**\n",
      "    *   **오픈 소스:** 모델의 가중치가 공개되어 있어 누구나 다운로드하여 자신의 서버나 기기에서 실행하고 수정할 수 있습니다. 상업적 사용도 가능합니다.\n",
      "    *   **경량/효율성:** 작은 크기 덕분에 비교적 적은 컴퓨팅 자원으로도 실행할 수 있어, 개인 컴퓨터, 모바일 기기, 엣지 디바이스 등 다양한 환경에서 활용하기 용이합니다.\n",
      "    *   **크기 대비 뛰어난 성능:** 작은 모델임에도 불구하고 Google의 연구 노하우가 집약되어 있어, 동급 모델 대비 뛰어난 성능을 보여줍니다. 주로 텍스트 기반 작업에 특화되어 있습니다.\n",
      "*   **주요 활용 분야:** 개인 개발자의 프로젝트, 소규모 기업의 맞춤형 AI 애플리케이션, 온디바이스 AI, 연구 및 교육 목적.\n",
      "*   **비유:** 개발자를 위한 **고성능 미니카** 또는 **실용적인 도구 세트**.\n",
      "\n",
      "---\n",
      "\n",
      "### 요약 비교표\n",
      "\n",
      "| 특성       | Gemini (제미니)                                   | Gemma (젬마)                                             |\n",
      "| :--------- | :------------------------------------------------ | :------------------------------------------------------- |\n",
      "| **개발 목적**  | 최고 성능의 범용 AI 모델 (플래그십)                   | 개발자/연구자를 위한 경량, 오픈 소스 모델 (접근성, 효율성) |\n",
      "| **모델 규모**  | 매우 큼 (최고 성능 지향)                         | 비교적 작음 (2B, 7B 등)                                  |\n",
      "| **핵심 특징**  | 멀티모달, 최고 성능, 복잡한 추론/생성             | 오픈 소스, 경량/효율성, 커스터마이징 용이                |\n",
      "| **접근성/라이선스** | API 제공 (폐쇄형), Google 서비스 통합              | 모델 가중치 공개 (오픈 소스), 자유로운 다운로드 및 사용 |\n",
      "| **주요 활용**  | Google 서비스, 대규모/복잡한 AI 솔루션            | 개인 프로젝트, 온디바이스 AI, 커스텀 AI 개발, 연구      |\n",
      "| **성능 수준**  | 현존 최고 수준                                  | 크기 대비 매우 뛰어남 (동급 최고 수준)                  |\n",
      "| **비유**     | Google의 '주력 전투기' 또는 '최고급 스포츠카'       | 개발자를 위한 '고성능 미니카' 또는 '실용적인 도구 세트' |\n",
      "\n",
      "---\n",
      "\n",
      "결론적으로, Gemini는 Google의 최첨단 AI 기술력을 보여주는 **\"최고의 결과물\"**이고, Gemma는 Gemini 개발 과정에서 얻은 기술적 노하우를 바탕으로 **\"AI 생태계를 확장하고 개발자들에게 더 많은 기회를 제공\"**하기 위해 만들어진 모델이라고 할 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5cd82f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Gemini와 Gemma는 모두 Google이 개발한 AI 모델이지만, **목표, 규모, 활용 방식, 그리고 라이선스에서 큰 차이**가 있습니다. 비유하자면, Gemini는 '최고급 플래그십 전투기'이고, Gemma는 '개발자를 위한 고성능 미니카'라고 할 수 있습니다.\n",
       "\n",
       "자세한 차이점은 다음과 같습니다.\n",
       "\n",
       "---\n",
       "\n",
       "### 1. Gemini (제미니)\n",
       "\n",
       "*   **개발 목적:** Google의 가장 강력하고 최첨단 AI 모델로, **다양한 복잡한 작업을 수행하는 범용 인공지능**을 목표로 합니다.\n",
       "*   **모델 크기/규모:** 매우 크고 복잡한 모델로, 여러 가지 크기(Nano, Pro, Ultra)가 있지만, 일반적으로 **최고 성능을 위한 대규모 모델**을 지칭합니다.\n",
       "*   **주요 특징:**\n",
       "    *   **멀티모달(Multimodal):** 텍스트, 이미지, 오디오, 비디오 등 다양한 형태의 정보를 이해하고 생성할 수 있습니다.\n",
       "    *   **최고 성능:** 현존하는 AI 모델 중 최고 수준의 성능을 자랑하며, 복잡한 추론, 코딩, 창의적인 콘텐츠 생성 등에 강합니다.\n",
       "    *   **폐쇄형/API 접근:** 주로 Google의 서비스(예: Gemini Advanced, Google Cloud Vertex AI)를 통해 API 형태로 접근하거나, Google 제품에 통합되어 사용됩니다. 모델의 가중치 등 세부 정보는 공개되지 않습니다.\n",
       "*   **주요 활용 분야:** Google의 AI 기반 서비스(Bard/Gemini Advanced), 복잡한 기업용 애플리케이션, 연구 개발의 최전선.\n",
       "*   **비유:** Google의 **주력 전투기** 또는 **최고급 스포츠카**.\n",
       "\n",
       "### 2. Gemma (젬마)\n",
       "\n",
       "*   **개발 목적:** Google이 개발한 **경량, 오픈 소스 AI 모델 패밀리**입니다. 개발자와 연구자들이 쉽게 접근하고 활용하며, 특정 용도에 맞게 미세 조정(fine-tuning)할 수 있도록 설계되었습니다.\n",
       "*   **모델 크기/규모:** Gemini보다 훨씬 작은 규모로 제공됩니다 (예: 2B, 7B). 이는 **효율성과 접근성**에 중점을 둔 결과입니다.\n",
       "*   **주요 특징:**\n",
       "    *   **오픈 소스:** 모델의 가중치가 공개되어 있어 누구나 다운로드하여 자신의 서버나 기기에서 실행하고 수정할 수 있습니다. 상업적 사용도 가능합니다.\n",
       "    *   **경량/효율성:** 작은 크기 덕분에 비교적 적은 컴퓨팅 자원으로도 실행할 수 있어, 개인 컴퓨터, 모바일 기기, 엣지 디바이스 등 다양한 환경에서 활용하기 용이합니다.\n",
       "    *   **크기 대비 뛰어난 성능:** 작은 모델임에도 불구하고 Google의 연구 노하우가 집약되어 있어, 동급 모델 대비 뛰어난 성능을 보여줍니다. 주로 텍스트 기반 작업에 특화되어 있습니다.\n",
       "*   **주요 활용 분야:** 개인 개발자의 프로젝트, 소규모 기업의 맞춤형 AI 애플리케이션, 온디바이스 AI, 연구 및 교육 목적.\n",
       "*   **비유:** 개발자를 위한 **고성능 미니카** 또는 **실용적인 도구 세트**.\n",
       "\n",
       "---\n",
       "\n",
       "### 요약 비교표\n",
       "\n",
       "| 특성       | Gemini (제미니)                                   | Gemma (젬마)                                             |\n",
       "| :--------- | :------------------------------------------------ | :------------------------------------------------------- |\n",
       "| **개발 목적**  | 최고 성능의 범용 AI 모델 (플래그십)                   | 개발자/연구자를 위한 경량, 오픈 소스 모델 (접근성, 효율성) |\n",
       "| **모델 규모**  | 매우 큼 (최고 성능 지향)                         | 비교적 작음 (2B, 7B 등)                                  |\n",
       "| **핵심 특징**  | 멀티모달, 최고 성능, 복잡한 추론/생성             | 오픈 소스, 경량/효율성, 커스터마이징 용이                |\n",
       "| **접근성/라이선스** | API 제공 (폐쇄형), Google 서비스 통합              | 모델 가중치 공개 (오픈 소스), 자유로운 다운로드 및 사용 |\n",
       "| **주요 활용**  | Google 서비스, 대규모/복잡한 AI 솔루션            | 개인 프로젝트, 온디바이스 AI, 커스텀 AI 개발, 연구      |\n",
       "| **성능 수준**  | 현존 최고 수준                                  | 크기 대비 매우 뛰어남 (동급 최고 수준)                  |\n",
       "| **비유**     | Google의 '주력 전투기' 또는 '최고급 스포츠카'       | 개발자를 위한 '고성능 미니카' 또는 '실용적인 도구 세트' |\n",
       "\n",
       "---\n",
       "\n",
       "결론적으로, Gemini는 Google의 최첨단 AI 기술력을 보여주는 **\"최고의 결과물\"**이고, Gemma는 Gemini 개발 과정에서 얻은 기술적 노하우를 바탕으로 **\"AI 생태계를 확장하고 개발자들에게 더 많은 기회를 제공\"**하기 위해 만들어진 모델이라고 할 수 있습니다."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(response.content) # markdown으로 출력"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
