{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\playdata\\anaconda3\\envs\\dl\\lib\\site-packages (3.10.1)\n",
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.10.3-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: torchinfo in c:\\users\\playdata\\anaconda3\\envs\\dl\\lib\\site-packages (1.8.0)\n",
      "Collecting gdown\n",
      "  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\playdata\\anaconda3\\envs\\dl\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\playdata\\anaconda3\\envs\\dl\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\playdata\\anaconda3\\envs\\dl\\lib\\site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\playdata\\anaconda3\\envs\\dl\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\playdata\\anaconda3\\envs\\dl\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\playdata\\anaconda3\\envs\\dl\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\playdata\\anaconda3\\envs\\dl\\lib\\site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\playdata\\anaconda3\\envs\\dl\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\playdata\\anaconda3\\envs\\dl\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\playdata\\anaconda3\\envs\\dl\\lib\\site-packages (from gdown) (4.13.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\playdata\\anaconda3\\envs\\dl\\lib\\site-packages (from gdown) (3.18.0)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\playdata\\anaconda3\\envs\\dl\\lib\\site-packages (from gdown) (2.32.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\playdata\\anaconda3\\envs\\dl\\lib\\site-packages (from gdown) (4.67.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\playdata\\anaconda3\\envs\\dl\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\playdata\\anaconda3\\envs\\dl\\lib\\site-packages (from beautifulsoup4->gdown) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\playdata\\anaconda3\\envs\\dl\\lib\\site-packages (from beautifulsoup4->gdown) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\playdata\\anaconda3\\envs\\dl\\lib\\site-packages (from requests[socks]->gdown) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\playdata\\anaconda3\\envs\\dl\\lib\\site-packages (from requests[socks]->gdown) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\playdata\\anaconda3\\envs\\dl\\lib\\site-packages (from requests[socks]->gdown) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\playdata\\anaconda3\\envs\\dl\\lib\\site-packages (from requests[socks]->gdown) (2025.4.26)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6 (from requests[socks]->gdown)\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\playdata\\anaconda3\\envs\\dl\\lib\\site-packages (from tqdm->gdown) (0.4.6)\n",
      "Using cached matplotlib-3.10.3-cp312-cp312-win_amd64.whl (8.1 MB)\n",
      "Downloading gdown-5.2.0-py3-none-any.whl (18 kB)\n",
      "Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: PySocks, matplotlib, gdown\n",
      "\n",
      "  Attempting uninstall: matplotlib\n",
      "\n",
      "    Found existing installation: matplotlib 3.10.1\n",
      "\n",
      "    Uninstalling matplotlib-3.10.1:\n",
      "\n",
      "   ------------- -------------------------- 1/3 [matplotlib]\n",
      "   ------------- -------------------------- 1/3 [matplotlib]\n",
      "   ------------- -------------------------- 1/3 [matplotlib]\n",
      "   ------------- -------------------------- 1/3 [matplotlib]\n",
      "   ------------- -------------------------- 1/3 [matplotlib]\n",
      "   ------------- -------------------------- 1/3 [matplotlib]\n",
      "      Successfully uninstalled matplotlib-3.10.1\n",
      "   ------------- -------------------------- 1/3 [matplotlib]\n",
      "   ------------- -------------------------- 1/3 [matplotlib]\n",
      "   ------------- -------------------------- 1/3 [matplotlib]\n",
      "   ------------- -------------------------- 1/3 [matplotlib]\n",
      "   ------------- -------------------------- 1/3 [matplotlib]\n",
      "   ------------- -------------------------- 1/3 [matplotlib]\n",
      "   ------------- -------------------------- 1/3 [matplotlib]\n",
      "   ------------- -------------------------- 1/3 [matplotlib]\n",
      "   ------------- -------------------------- 1/3 [matplotlib]\n",
      "   ------------- -------------------------- 1/3 [matplotlib]\n",
      "   ------------- -------------------------- 1/3 [matplotlib]\n",
      "   ------------- -------------------------- 1/3 [matplotlib]\n",
      "   ------------- -------------------------- 1/3 [matplotlib]\n",
      "   ------------- -------------------------- 1/3 [matplotlib]\n",
      "   ------------- -------------------------- 1/3 [matplotlib]\n",
      "   ---------------------------------------- 3/3 [gdown]\n",
      "\n",
      "Successfully installed PySocks-1.7.1 gdown-5.2.0 matplotlib-3.10.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install matplotlib torchinfo gdown -U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZEiUymWcbfRZ",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Pretrained Model\n",
    "\n",
    "-   미리 학습된 모델.\n",
    "-   **Pretrained model을 이용해 현재 문제를 해결한다.**\n",
    "    -   새로 모델을 정의 하고 학습시키는 것 보다 Pretrained 모델을 사용해 모델을 정의하면 훨씬 좋은 성능의 모델을 만들 수 있다.\n",
    "-   **Pretrain model을 사용하는 방식**\n",
    "    -   제로샷 전이학습(Zero shot transfer learning)\n",
    "        -   추가 학습없어 Pretrained 모델을 해결하려는 문제에 그대로 사용한다.\n",
    "    -   전이학습 (Transfer learning)\n",
    "        -   Pretrained 모델의 일부분을 재학습 시킨다. 주로 출력 Layer를 학습시킨다.\n",
    "    -   미세조정 (Fine tuning)\n",
    "        -   Pretrained 모델의 파라미터를 초기 파라미터로 사용하여 Custom Dataset으로 학습을 진행하여 모델의 모든 파라미터를 업데이트 시킨다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AFtMH4PMWGCa"
   },
   "source": [
    "## Pytorch에서 제공하는 Pretrained Model\n",
    "\n",
    "-   분야별 라이브러리에서 제공\n",
    "    -   torchvision: https://pytorch.org/vision/stable/models.html\n",
    "-   torch hub 를 이용해 모델과 학습된 parameter를 사용할 수 있다.\n",
    "    -   https://pytorch.org/hub/\n",
    "-   이외에도 많은 모델과 학습된 paramter가 인터넷상에 공개되 있다.\n",
    "    -   딥러닝 모델기반 application을 개발 할 때는 대부분 Transfer Learning을 한다.\n",
    "    -   다양한 분야에서 연구된 많은 딥러닝 모델들이 구현되어 공개 되어 있으며 학습된 Parameter들도 제공되고 있다.\n",
    "    -   [paperswithcode](https://paperswithcode.com/)에서 State Of The Art(SOTA) 논문들과 그 구현된 모델을 확인할 수 있다.\n",
    "\n",
    "> **State Of The Art(SOTA)**: 특정 시점에 특정 분야에서 가장 성능이 좋은 모델을 말한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RRSB293RWGCa"
   },
   "source": [
    "## VGGNet Pretrained 모델을 이용해 이미지 분류\n",
    "\n",
    "-   Pytorch가 제공하는 VGG 모델은 ImageNet dataset으로 사전 학습 시킨 모델로 그 학습된 parameter를 제공한다.\n",
    "    -   120만장의 trainset, 1000개의 class로 구성된 데이터셋.\n",
    "    -   Output으로 1000개의 카테고리에 대한 확률을 출력한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wget\n",
      "  Downloading wget-3.2.zip (10 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: wget\n",
      "  Building wheel for wget (setup.py): started\n",
      "  Building wheel for wget (setup.py): finished with status 'done'\n",
      "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9713 sha256=ba5651d63cbce00d024ff7e269644b6090d7e3a2939781cba0a8e09480dd74cf\n",
      "  Stored in directory: c:\\users\\playdata\\appdata\\local\\pip\\cache\\wheels\\01\\46\\3b\\e29ffbe4ebe614ff224bad40fc6a5773a67a163251585a13a9\n",
      "Successfully built wget\n",
      "Installing collected packages: wget\n",
      "Successfully installed wget-3.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: Building 'wget' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'wget'. Discussion can be found at https://github.com/pypa/pip/issues/6334\n"
     ]
    }
   ],
   "source": [
    "# ImageNet 1000개의 class 목록\n",
    "%pip install wget\n",
    "import wget\n",
    "url = 'https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a/raw/238f720ff059c1f82f368259d1ca4ffa5dd8f9f5/imagenet1000_clsidx_to_labels.txt'\n",
    "imagenet_filepath = wget.download(url) # url의 파일을 다운로드."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'> 1000\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "with open(\"imagenet1000_clsidx_to_labels.txt\", \"rt\") as fr:\n",
    "    index_to_class = ast.literal_eval(fr.read())  # dictionary로 변환.\n",
    "print(type(index_to_class), len(index_to_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tench, Tinca tinca\n",
      "1 goldfish, Carassius auratus\n",
      "2 great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias\n",
      "3 tiger shark, Galeocerdo cuvieri\n",
      "4 hammerhead, hammerhead shark\n"
     ]
    }
   ],
   "source": [
    "for key, value in list(index_to_class.items())[:5]:\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained Model Load 및 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms  # torchvision.models: Pretrained 모델들을 제공.\n",
    "from torchinfo import summary\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pretrained 모델 Loading\n",
    "load_model = models.vgg16(\n",
    "    weights=models.VGG16_Weights.DEFAULT # 학습된 weight(parameter) 도 같이 load\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추론할 이미지 다운로드\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "img_url = 'https://cdn.download.ams.birds.cornell.edu/api/v1/asset/169231441/1800'\n",
    "img_url = 'https://blogs.ifas.ufl.edu/news/files/2021/10/anole-FB.jpg'\n",
    "img_url = 'https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/YellowLabradorLooking_new.jpg/640px-YellowLabradorLooking_new.jpg'\n",
    "\n",
    "res = requests.get(img_url)\n",
    "# res.content # binary data\n",
    "# BytesIO() -> binary data(file)을 bytes 타입으로 변환.\n",
    "\n",
    "test_img = Image.open(BytesIO(res.content)) # res: http 응답정보. res.content: 다운받은 binary 파일\n",
    "test_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "input_tensor = transform(test_img).unsqueeze(0)\n",
    "input_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = load_model.to(device)\n",
    "input_tensor = input_tensor.to(device)\n",
    "\n",
    "load_model.eval()\n",
    "with torch.no_grad():\n",
    "    pred = load_model(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = torch.nn.Softmax(dim=-1)(pred)\n",
    "pred_cls = pred.max(dim=-1).indices[0]\n",
    "pred_proba = pred.max(dim=-1).values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred_proba)\n",
    "print(pred_cls, index_to_class[pred_cls.item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_sLmJGHWGCi"
   },
   "source": [
    "# Transfer learning (전이학습)\n",
    "\n",
    "-   사전에 학습된 신경망의 구조와 파라미터를 재사용해서 새로운 모델(우리가 만드는 모델)의 시작점으로 삼고 해결하려는 문제를 위해 다시 학습시킨다.\n",
    "-   전이 학습을 통해 다음을 해결할 수 있다.\n",
    "    1. 데이터 부족문제\n",
    "        - 딥러닝은 대용량의 학습데이터가 필요하다.\n",
    "        - 충분한 데이터를 수집하는 것은 항상 어렵다.\n",
    "    2. 과다한 계산량\n",
    "        - 신경망 학습에는 엄청난 양의 계산 자원이 필요하다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cj9RRN6QbfRR"
   },
   "source": [
    "![transfer_learning01](figures/09_transfer_01.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5dXETWizbfRS"
   },
   "source": [
    "-   미리 학습된(pre-trained) Model을 이용하여 모델을 구성한 뒤 현재 하려는 예측 문제를 해결한다.\n",
    "-   보통 Pretrained Model에서 Feature Extraction 부분을 사용한다.\n",
    "    -   Computer Vision 문제의 경우 Bottom 쪽의 Convolution Layer(Feature Extractor)들은 이미지에 나타나는 일반적인 특성을 추출하므로 **다른 대상을 가지고 학습했다고 하더라도 재사용할 수 있다.**\n",
    "    -   Top 부분 Layer 부분은 특히 출력 Layer의 경우 대상 데이터셋의 목적에 맞게 변경 해야 하므로 재사용할 수 없다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZnS6S0QPbfRT"
   },
   "source": [
    "![transfer_learning02](figures/09_transfer_02.png)\n",
    "\n",
    "> **Frozon**: Training시 parameter가 update 되지 않도록 하는 것을 말한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NRLhG6D8bfRb"
   },
   "source": [
    "### Feature extraction 재사용\n",
    "\n",
    "-   Pretrained Model에서 Feature Extractor 만 가져오고 추론기(Fully connected layer)만 새로 정의한 뒤 그 둘을 합쳐서 모델을 만든다.\n",
    "-   학습시 직접 구성한 추론기만 학습되도록 한다.\n",
    "    -   Feature Extractor는 추론을 위한 Feature 추출을 하는 역할만 하고 그 parameter(weight)가 학습되지 않도록 한다.\n",
    "-   모델/레이어의 parameter trainable 여부 속성 변경\n",
    "    -   model/layer 의 `parameters()` 메소드를 이용해 weight와 bias를 조회한 뒤 `requires_grad` 속성을 `False`로 변경한다.\n",
    "\n",
    "#### Backbone, Base network\n",
    "\n",
    "-   전체 네트워크에서 Feature Extraction의 역할을 담당하는 부분을 Backbone/Base network라고 한다.\n",
    "-   추론을 담당하는 layer block은 Head Network 이라고 한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "miqV_0WSbfRm"
   },
   "source": [
    "## Fine-tuning(미세조정)\n",
    "\n",
    "-   Transfer Learning을 위한 Pretrained 모델을 내가 학습시켜야 하는 데이터셋(Custom Dataset)으로 **재학습**시키는 것을 fine tuning 이라고 한다.\n",
    "-   주어진 문제에 더 적합하도록 Feature Extractor의 가중치들도 조정 한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "py7c3rmmbfRm"
   },
   "source": [
    "### Fine tuning 전략\n",
    "\n",
    "![transfer02](figures/09_transfer_03.png)\n",
    "\n",
    "-   **세 전략 모두 추론기는 trainable로 한다.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4wtxXsYUbfRn"
   },
   "source": [
    "**<span style='font-size:1.2em'>1. 전체 모델을 전부 학습시킨다.(1번)</span>**\n",
    "\n",
    "-   Pretrained 모델의 weight는 Feature extraction 의 초기 weight 역할을 한다.\n",
    "-   **Train dataset의 양이 많고** Pretrained 모델이 학습했던 dataset과 Custom dataset의 class간의 유사성이 **낮은 경우** 적용.\n",
    "-   학습에 시간이 많이 걸린다.\n",
    "\n",
    "**<span style='font-size:1.2em'>2. Pretrained 모델 Bottom layer들(Input과 가까운 Layer들)은 고정시키고 Top layer의 일부를 재학습시킨다.<span>**\n",
    "\n",
    "-   **Train dataset의 양이 많고** Pretrained 모델이 학습했던 dataset과 Custom dataset의 class간의 유사성이 **높은 경우** 적용.\n",
    "-   **Train dataset의 양이 적고** Pretained 모델이 학습했던 dataset과 custom dataset의 class간의 유사성이 **낮은 경우** 적용\n",
    "\n",
    "**<span style='font-size:1.2em'>3. Pretrained 모델 전체를 고정시키고 classifier layer들만 학습시ont3번)</span>**\n",
    "\n",
    "-   **Train dataset의 양이 적고** Pretrained 모델이 학습했던 dataset과 Custom dataset의 class간의 유사성이 **높은 경우** 적용.\n",
    "\n",
    "> **Custom dataset:** 내가 학습시키고자 하는 dataset\n",
    "> 1번 2번 전략을 Fine tuning 이라고 한다.\n",
    "\n",
    "![fine tuning](figures/09_finetuning.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gdown -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import models, datasets, transforms\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "from module.train import fit, test_multi_classification\n",
    "from module.utils import plot_fit_result\n",
    "\n",
    "import os\n",
    "from zipfile import ZipFile\n",
    "\n",
    "\n",
    "import gdown\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"data\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download\n",
    "url = \"https://drive.google.com/uc?id=1YIxDL0XJhhAMdScdRUfDgccAqyCw5-ZV\"\n",
    "path = r\"data/cats_and_dogs_small.zip\" # 다운 받은 공유파일을 저장할 경로.\n",
    "gdown.download(url, path, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 압축 풀기\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "target_path = 'data/cats_and_dogs'\n",
    "with ZipFile(path) as zf:\n",
    "    zf.extractall(target_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9x8yDSiBWGCm"
   },
   "outputs": [],
   "source": [
    "target_path = 'data/cats_and_dogs'\n",
    "BATCH_SIZE = 64\n",
    "EPOCH = 10\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k8EqrXFlWGCn"
   },
   "outputs": [],
   "source": [
    "# Image Augmentation+전처리 설정\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), # resize\n",
    "    transforms.ToTensor(),         # ndarray, Image -> Tensor, 0 ~ 1 정규화, channel first\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    # 채널별 평균, 표준편차설정. -> Standard Scaling 처리.(픽셀값-평균)/표준편차\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(), \n",
    "    transforms.RandomRotation(degrees=180)\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), #  resize\n",
    "    transforms.ToTensor(),          # ndarray, Image -> Tensor, 0 ~ 1 정규화, channel first\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 정의\n",
    "train_set = datasets.ImageFolder(\n",
    "    os.path.join(target_path, \"train\"), # Data들을 저장한 디렉토리.\n",
    "    transform=train_transform\n",
    ")\n",
    "valid_set = datasets.ImageFolder(\n",
    "    os.path.join(target_path, \"validation\"),\n",
    "    transform=test_transform\n",
    ")\n",
    "test_set = datasets.ImageFolder(\n",
    "    os.path.join(target_path, \"test\"),\n",
    "    transform=test_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class, class_name 확인\n",
    "print(train_set.classes)\n",
    "print(train_set.class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, drop_last=True,  \n",
    "                          num_workers=os.cpu_count()) # 데이터 불러오는 것 병렬처리.\n",
    "valid_loader = DataLoader(valid_set, batch_size=BATCH_SIZE, num_workers=os.cpu_count())\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, num_workers=os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 수\n",
    "len(train_loader), len(test_loader), len(valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Trainset Image 확인\n",
    "import matplotlib.pyplot as plt\n",
    "img = train_set[0][0]\n",
    "img = img.permute(1, 2, 0)\n",
    "img.shape\n",
    "print(train_set[0][1])\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 정의, 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer Learning - Backbone 모델: VGG16 + classifier(내것)\n",
    "model = models.vgg16(models.VGG16_Weights.DEFAULT)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frozon -> 파라미터들이 학습시 update 되지 않도록 변경.\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분류기(classifier)를 내것으로 변경.\n",
    "model.classifier = nn.Linear(in_features=25088, out_features=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model, (1, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"saved_models\", exist_ok=True)\n",
    "save_model_path = \"saved_models/cat_dog_model.pt\"\n",
    "\n",
    "model = model.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "result = fit(train_loader, valid_loader, model, loss_fn, optimizer, EPOCH,\n",
    "            save_best_model=True, save_model_path=save_model_path, \n",
    "            device=device, mode=\"multi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 최종평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = torch.load(save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = test_multi_classification(\n",
    "    test_loader, load_model, loss_fn, device=device\n",
    ")\n",
    "loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from PIL import Image\n",
    "\n",
    "def predict(image_path, model, transform, device):\n",
    "    # \"model로 image_path의 이미지를 추론한 결과를 반환.\"\n",
    "    img = Image.open(image_path)  # 추론대상 이미지 loading\n",
    "    input_data = transform(img)  # shape: (C, H, W)\n",
    "    input_data = input_data.unsqueeze(dim=0) # (C, H, W) -> (1, C, H, W) \n",
    "    input_data = input_data.to(device)\n",
    "\n",
    "    # 추론\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model(input_data)\n",
    "        pred_proba = pred.softmax(dim=-1) # 확률값으로 변경.\n",
    "        pred_label = pred_proba.argmax(dim=-1).item()  # Tensor([3]) -> 3\n",
    "        pred_proba_max = pred_proba.max(dim=-1).values.item()\n",
    "        class_name = \"cat\" if pred_label == 0 else \"dog\"\n",
    "        return pred_label, class_name, pred_proba_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"test_img/dog.jpg\", load_model, test_transform, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"test_img/cat.jpg\", load_model, test_transform, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"test_img/image.jpg\", load_model, test_transform, device)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [
    "ZEiUymWcbfRZ"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "hide_input": false,
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "551.4px",
    "left": "1166px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
